# <div align=center> Korean OCR based on Clova AI Deep Text Recognition <br/> using AI Hub Data </div>

<div align=right> <img alt="GitHub repo size" src="https://img.shields.io/github/repo-size/HJK02130/Korean-OCR-based-on-Clova-AI-Deep-Text-Recognition-using-AI-Hub-Data?style=flat-square"> <img alt="GitHub code size in bytes" src="https://img.shields.io/github/languages/code-size/HJK02130/Korean-OCR-based-on-Clova-AI-Deep-Text-Recognition-using-AI-Hub-Data?style=flat-square"> <img alt="GitHub language count" src="https://img.shields.io/github/languages/count/HJK02130/Korean-OCR-based-on-Clova-AI-Deep-Text-Recognition-using-AI-Hub-Data?style=flat-square"> </div>


## Contents
1. [Overview](#overview)
2. [Issue](#issue)
3. [Environment](#environment)
4. [Requirements](#requirements)
5. [Usage](#usage)
6. [Repository Explaination](#repository-explaination)
7. [Architecture](#architecture)
8. [Result](#result)
9. [Conclusion](#conclusion)
10. [Reference](#reference)
11. [Developer](#developer)


## Overview
[[DACON contest]](https://dacon.io/competitions/official/235970/overview/description)  [[Clova AI Deep Text Recognition Benchmark]](https://github.com/clovaai/deep-text-recognition-benchmark) <br/><br/>
OCR technology can greatly reduce the time and effort required to scan documents into images, simplify the operations required to convert text images into data, and increase productivity by automating the process. This project was carried out in the SW-centeral university joint AI competition <finals> conducted by DACON. We developed an 'Optical Character Recognition (OCR)' algorithm that can detect and recognize text from images in which the text part is cropped out of signboards, book covers, and sign images of Hangul text. Using the Korean text image data provided by DACON, the original Korean text image data provided by AI Hub, and the text image we created, we trained the [Korean OCR deep learning model](https://github.com/clovaai/deep-text-recognition-benchmark) provided by Clova AI, and use the accuracy as a performance evaluation metrics. However, because the PC environment was not good and the speed was slow, only 18900 of 300000 epochs, that is, 6.3% of the scheduled epochs were trained and we forced training to end before the training is completed. Also, we didn't used all data we collected. Please note this. As a result of the test, DACON's public test set showed an accuracy of 0.539, and the private test set showed an accuracy of 0.523. We believe that better results could be achieved if training was completed using all the data collected and a better GPU without hardware limitations.
<br/>
<br/>
OCR ê¸°ìˆ ì€ ë¬¸ì„œë¥¼ ì´ë¯¸ì§€ë¡œ ìŠ¤ìº”í•˜ëŠ” ì‘ì—…ì— ë“¤ì–´ê°€ëŠ” ì‹œê°„ê³¼ ë…¸ë ¥ì„ í¬ê²Œ ë‹¨ì¶•ì‹œí‚¬ ìˆ˜ ìˆìœ¼ë©°, í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ë° í•„ìš”í•œ ìš´ì˜ì„ ê°„ì†Œí™”í•˜ê³  í”„ë¡œì„¸ìŠ¤ë¥¼ ìë™í™”í•˜ì—¬ ìƒì‚°ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³¸ í”„ë¡œì íŠ¸ëŠ” DACONì—ì„œ ì§„í–‰í•œ SWì¤‘ì‹¬ëŒ€í•™ ê³µë™ AI ê²½ì§„ëŒ€íšŒ <ë³¸ì„ >ì—ì„œ ì§„í–‰ëœ í”„ë¡œì íŠ¸ì´ë©°, í•œê¸€ í…ìŠ¤íŠ¸ì˜ ê°„íŒ, ì±…í‘œì§€, í‘œì§€íŒ ì´ë¯¸ì§€ ì¤‘ í…ìŠ¤íŠ¸ ë¶€ë¶„ì´ cropëœ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ íƒì§€í•˜ê³  ì¸ì‹í•  ìˆ˜ ìˆëŠ” 'ê´‘í•™ ë¬¸ì ì¸ì‹(Optical Character Recognition, OCR)'ì„ ì£¼ì œë¡œ ì•Œê³ ë¦¬ì¦˜ì„ ê°œë°œí•˜ì˜€ìŠµë‹ˆë‹¤. DACONì—ì„œ ì œê³µí•˜ëŠ” í•œê¸€ í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ ë°ì´í„°, AI Hubì—ì„œ ì œê³µí•˜ëŠ” í•œê¸€ í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ ì›ì²œë°ì´í„°, ê·¸ë¦¬ê³  KAISTì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ì™€ ì§ì ‘ ìƒì„±í•œ í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ í™œìš©í•˜ì—¬ Clova AIì—ì„œ ì œê³µí•˜ëŠ” í•œê¸€ OCR ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³ , ì„±ëŠ¥í‰ê°€ì§€í‘œë¡œ ì •í™•ë„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ í‰ê°€ë¥¼ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. ë‹¨, PC í™˜ê²½ì´ ì¢‹ì§€ ì•Šì•„ ì†ë„ê°€ ëŠë¦° ê´€ê³„ë¡œ ì˜ˆì •ë˜ì—ˆë˜ Epoch 300000 ì¤‘ 18900 ì¦‰, ì˜ˆì • Epochì˜ 6.3%ë§Œ í•™ìŠµì‹œí‚¤ê³  ê°•ì œë¡œ í•™ìŠµì„ ì¢…ë£Œí•˜ì˜€ê³ , ìˆ˜ì§‘í•œ ëª¨ë“  ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ëª»í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ ì  ì°¸ê³ í•´ì£¼ì‹œê¸¸ ë°”ëë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ê²°ê³¼, DACONì˜ public test setì—ì„œëŠ” 0.539ì˜ ì •í™•ë„ë¥¼, private test setì—ì„œëŠ” 0.523ì˜ ì •í™•ë„ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤. ë¹„êµì  ì„±ëŠ¥ì´ ì¢‹ì€ GPUë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì§‘ ë° ìƒì„±í•œ ëª¨ë“  ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ í•˜ë“œì›¨ì–´ ì œí•œ ì—†ì´ í•™ìŠµì„ ì™„ë£Œí•˜ì˜€ë‹¤ë©´ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆì„ ê²ƒì´ë¼ê³  íŒë‹¨í•©ë‹ˆë‹¤.

## Issue
!! We didn't used all data we collected<br/>
!! We didn't do training to the end<br/>
!! You need better GPU<br/>
!! It takes a long time

## Environment
+ Pytorch 1.3.1
+ CUDA 10.1
+ Python 3.6

## Requirements
+ lmbd
+ pillow
+ torchvision
+ nltk
+ natsort
+ tqdm

## Usage
Filetree (modifying)

## Repository Explaination
###### ğŸ“ deep-text-recognition-benchmark<br/>code folder
> ###### ğŸ“„ json_to_txt.py<br/>Take text in info file(.json) and each image path and create a text file(.txt)
> ###### ğŸ“„ modify_txt.ipynb<br/>Modify created text file to suitable format for making lmdb data
> ###### ğŸ“„ create_lmdb_dataset.py<br/>Clova AI's deep text recognition - code creating lmdb data
> ###### ğŸ“„ train.py<br/>Clova AI deep text recognition - training code
> ###### ğŸ“„ test.py<br/>Clova AI deep text recognition - test code

###### ğŸ“ saved_models<br/> trained model
> ###### ğŸ“ None-VGG-BiLSTM-CTC-Seed1111<br/>We tried to test model NVBC(None-VGG-BiLSTM-CTC)
>> ###### ğŸ“„ best_norm_ED.pth<br/>The model we trained that have best norm ED
>> ###### ğŸ“„ best_accuracy.pth<br/>The model we trained that have best accuracy

###### ğŸ“ result<br/>lmdb data
> ###### ğŸ“ trainig<br/>training lmdb data(empty)
> ###### ğŸ“ validatdion<br/>validation lmdb data(empty)
<br/>

## Architecture
### Training Data
+ [DACON training data](https://dacon.io/competitions/official/235970/overview/description)
	+ Image : Text cropped text-in-the-wild image data
	+ Text file(.csv) : Text in image
+ [AI Hub data](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=105)
	+ Image : Text-in-the-wild image
	+ Info file(.json) : Text, text bounding box coordinate and etc.
+ Generated data
	+ We made new generated data with random Korean text image with random font, text impact and random background image
	+ [generator reference](https://github.com/Belval/TextRecognitionDataGenerator)
+ [Kaist data](http://www.iapr-tc11.org/mediawiki/index.php/KAIST_Scene_Text_Database)
	+ Text-in-the-wild image data
	
<br/>

### Preprocessing
#### Image preprocessing
+ AI Hub data 
	+ Create text cropped image using bounding box coordinate in info file
#### File preprocessing
+ DACON data
	+ Take text in text file(.csv) and each image path and create a text file(.txt)
+ AI Hub data
	+ Take text in info file(.json) and each image path and create a text file(.txt)
+ Generated data
	+ Create text file with each image path and each text of image
#### Create LMDB data
~~~
python deep-text-recognition-benchmark/create_lmdb_dataset.py \
--inputPath train \
--gtFile train/gt.txt \
--outputPath result/training
~~~

#### Split data
+ split training data to training, validation data
+ [DACON test data](https://dacon.io/competitions/official/235970/data)

<br/><br/>

### Training
We tried to train model NVBC(None-VGG-BiLSTM-CTC) and used pre-trained OCR model
~~~
python deep-text-recognition-benchmark/train.py \
--train_data result/training \
--valid_data result/validation \
--input_channel 1 \
--output_channel 256 \
--hidden_size 256 \
--Transformation None \
--FeatureExtraction VGG \
--SequenceModeling BiLSTM \
--Prediction CTC \
â€”saved_model korean_g2.pth
--character " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_\`abcdefghijklmnopqrstuvwxyz{|}~ê°€ê°ê°„ê°‡ê°ˆê°ê°‘ê°’ê°•ê°–ê°™ê°šê°›ê°œê°ê±€ê±°ê±±ê±´ê±·ê±¸ê²€ê²ê²ƒê²‰ê²Œê²¨ê²©ê²ªê²¬ê²°ê²¹ê²½ê³ê³„ê³ ê³¡ê³¤ê³§ê³¨ê³°ê³±ê³³ê³µê³¼ê´€ê´‘ê´œê´´êµ‰êµêµ¬êµ­êµ°êµ³êµ´êµµêµ¶êµ½ê¶ê¶Œê·€ê·œê· ê·¸ê·¹ê·¼ê¸€ê¸ê¸ˆê¸‰ê¸‹ê¸ê¸°ê¸´ê¸¸ê¹€ê¹…ê¹Šê¹Œê¹ê¹ê¹”ê¹œê¹ê¹¥ê¹¨êº¼êº¾ê»ê»ê»‘ê»˜ê»´ê¼¬ê¼­ê¼´ê¼¼ê¼½ê½‚ê½ƒê½‰ê½¤ê¾¸ê¿€ê¿ˆë€Œë„ëˆëŠëŒë“ë”ë—ëë¼ë‚Œë‚˜ë‚™ë‚šë‚œë‚ ë‚¡ë‚¨ë‚©ë‚«ë‚­ë‚®ë‚¯ë‚±ë‚³ë‚´ëƒ„ëƒ‰ëƒëƒ¥ë„ˆë„‰ë„ë„“ë„˜ë„£ë„¤ë„¥ë„·ë…€ë…ë…„ë…ë…•ë…¸ë…¹ë…¼ë†€ë†ˆë†ë†’ë†“ë†”ë‡Œë‡¨ëˆ„ëˆˆëˆ•ë‰˜ë‰´ëŠ„ëŠëŠ‘ëŠ”ëŠ˜ëŠ™ëŠ¥ëŠ¦ëŠ¬ë‹ˆë‹ë‹˜ë‹¤ë‹¥ë‹¦ë‹¨ë‹«ë‹¬ë‹­ë‹®ë‹´ë‹µë‹·ë‹¹ë‹¿ëŒ€ëŒëŒë”ë•ë˜ëœë¤ë¥ë§ë©ë®ë°ë¸ë„ë…ëˆëŒë•ë™ë¼ë˜ëœë‘ë‘‘ë‘˜ë‘ ë‘¡ë‘¥ë’¤ë’·ë“œë“ë“ ë“£ë“¤ë“¬ë“­ë“¯ë“±ë””ë”©ë”ªë”°ë”±ë”´ë”¸ë•€ë•…ë•Œë•œë– ë–¡ë–¤ë–¨ë–»ë–¼ë˜ë˜‘ëšœëš«ëš±ë›°ëœ¨ëœ©ëœ¯ëœ°ëœ»ë„ë¼ë½ë€ëŒëë‘ë—ë˜ëœë¨ë«ëµëŸ‰ëŸ¬ëŸ­ëŸ°ëŸ´ëŸ¼ëŸ½ëŸ¿ë ë ‡ë ˆë ‰ë Œë ¤ë ¥ë ¨ë ¬ë µë ¹ë¡€ë¡œë¡ë¡ ë¡¬ë¡­ë¡¯ë£Œë£¨ë£©ë£¹ë£»ë¤„ë¥˜ë¥™ë¥ ë¥­ë¥´ë¥¸ë¦„ë¦‡ë¦ë¦¬ë¦­ë¦°ë¦¼ë¦½ë¦¿ë§ˆë§‰ë§Œë§ë§ë§‘ë§˜ë§™ë§›ë§ë§ë§¡ë§£ë§¤ë§¥ë§¨ë§µë§ºë¨¸ë¨¹ë¨¼ë©€ë©ˆë©‹ë©ë©ë©”ë©˜ë©©ë©°ë©´ë©¸ëª…ëª‡ëª¨ëª©ëª°ëª¸ëª¹ëª»ëª½ë¬˜ë¬´ë¬µë¬¶ë¬¸ë¬»ë¬¼ë­„ë­‡ë­ë­£ë¯€ë¯¸ë¯¼ë¯¿ë°€ë°‰ë°Œë°ë°‘ë°”ë°•ë°–ë°˜ë°›ë°œë°ë°Ÿë°¤ë°¥ë°©ë°­ë°°ë°±ë±€ë±ƒë±‰ë²„ë²ˆë²Œë²”ë²•ë²—ë² ë²¤ë²¼ë²½ë³€ë³„ë³ë³‘ë³•ë³´ë³µë³¶ë³¸ë³¼ë´„ë´‡ë´‰ëµˆëµ™ë¶€ë¶ë¶„ë¶ˆë¶‰ë¶ë¶“ë¶•ë¶™ë·°ë¸Œë¸”ë¹„ë¹Œë¹—ë¹šë¹›ë¹ ë¹¨ë¹µë¹¼ëº¨ë»ë»”ë»—ë¼ˆë½‘ë¿Œë¿ì˜ì¨ì‚¬ì‚­ì‚°ì‚´ì‚¶ì‚¼ìƒìƒˆìƒ‰ìƒŒìƒì„œì„ì„ì„ ì„¤ì„¬ì„­ì„¯ì„±ì„¸ì„¼ì…ˆì…‹ì…˜ì†Œì†ì†ì†”ì†œì†Ÿì†¡ì†¥ì‡„ì‡ ì‡¼ìˆ˜ìˆ™ìˆœìˆ ìˆ¨ìˆ«ìˆ²ì‰¬ì‰½ìŠˆìŠ¤ìŠ¨ìŠ¬ìŠ´ìŠµìŠ·ìŠ¹ì‹œì‹ì‹ ì‹£ì‹¤ì‹«ì‹¬ì‹­ì‹±ì‹¶ì‹¸ì‹¹ìŒ€ìŒìŒ“ì¨ì©ì°ì¹ì„ì˜ìŸì‘¤ì“°ì“¸ì”€ì”Œì”¨ì”©ì”¬ì”¹ì”»ì•„ì•…ì•ˆì•‰ì•Šì•Œì•“ì•”ì••ì•—ì•™ì•ì• ì•¡ì•¼ì•½ì–‡ì–‘ì–—ì–˜ì–´ì–µì–¸ì–¹ì–»ì–¼ì—„ì—…ì—†ì—‡ì—‰ì—Œì—ì—ì—”ì—˜ì—¬ì—­ì—°ì—´ì—·ì—¼ì—½ì—¿ì˜ì˜†ì˜ˆì˜›ì˜¤ì˜¥ì˜¨ì˜¬ì˜®ì˜³ì˜·ì™€ì™„ì™•ì™œì™ ì™¸ì™¼ìš”ìš•ìš©ìš°ìš±ìš´ìš¸ì›€ì›ƒì›…ì›Œì›ì›”ì›¨ì›¬ìœ„ìœ—ìœ ìœ¡ìœ¨ìœ¼ìœ½ì€ì„ìŒì‘ì˜ì´ìµì¸ì¼ì½ìƒì„ì…ì‡ìˆìŠììì‘ì”ì–ì˜ì ì¡ì¥ì¦ì¬ìŸì €ì ì „ì ˆì Šì ì ‘ì “ì •ì –ì œì  ì ¯ì ¸ì¡°ì¡±ì¡´ì¡¸ì¢€ì¢ì¢…ì¢‹ì¢Œì£„ì£¼ì£½ì¤€ì¤„ì¤Œì¤ì¤‘ì¥ì¦ˆì¦‰ì¦Œì¦ì¦˜ì¦ì§€ì§ì§„ì§ˆì§ì§‘ì§“ì§•ì§™ì§šì§œì§ì§§ì§¸ì¨Œì©Œì©ì©ìª½ì«“ì­ˆì­‰ì°Œì°ì°¢ì°¨ì°©ì°¬ì°®ì°°ì°¸ì°½ì°¾ì±„ì±…ì±”ì±™ì²˜ì²™ì²œì² ì²«ì²­ì²´ì³ì´ˆì´‰ì´Œì´ì´¬ìµœì¶”ì¶•ì¶˜ì¶œì¶¤ì¶¥ì¶§ì¶©ì·¨ì¸ ì¸¡ì¸°ì¸µì¹˜ì¹™ì¹œì¹ ì¹¨ì¹­ì¹´ì¹¸ì¹¼ìºìº ì»¤ì»¨ì»¬ì»´ì»µì»·ì¼“ì¼œì½”ì½œì½¤ì½©ì¾Œì¿ í€´í¬í°í´í¼í‚¤í‚¬íƒ€íƒíƒ„íƒˆíƒ‘íƒ“íƒ•íƒœíƒíƒ¤í„°í„±í„¸í……í…Œí…í…”í…œí† í†¤í†±í†µí‡´íˆ¬íˆ¼í‰íŠ€íŠœíŠ¸íŠ¹íŠ¼íŠ¿í‹€í‹ˆí‹°í‹±íŒ€íŒ…íŒŒíŒíŒíŒ”íŒ¨íŒ©íŒ¬í¼í½í˜í´í¸í¼í‰íí¬í­í‘œí‘¸í‘¹í’€í’ˆí’í“¨í”„í”Œí””í”¼í”½í•„í•í•‘í•˜í•™í•œí• í•¨í•©í•­í•´í•µí•¸í–„í–‡í–‰í–¥í—ˆí—Œí—˜í—¤í—¬í˜€í˜„í˜ˆí˜‘í˜•í˜œí˜¸í˜¹í˜¼í™€í™í™”í™•í™˜í™œí™©íšŒíšíšŸíš¨í›„í›ˆí›Œí›”í›¨íœ˜íœ´í‰íí‘í”í˜í™í¡í¥í©í¬í°íˆí˜" â€”FT
~~~

<br/>

### Test
~~~
python deep-text-recognition-benchmark/test.py \
--eval_data result/validation --benchmark_all_eval \
--Transformation None --FeatureExtraction VGG --SequenceModeling BiLSTM --Prediction CTC \
--saved_model saved_models/None-VGG-BiLSTM-CTC-Seed1111/best_accuracy.pth
~~~

## Result
!! We didn't train model to the end !!
||Public test set|Private test set|
|:---:|:---|:---|
|Accuracy|0.53909|0.52316|

If you have better GPU and lots of time to train, you will undoubtedly be able to achieve near-perfect accuracy.

## Conclusion
Using Clova AI text recognition with deep learning methods in this DACON contest, It was able to learn a wide range of data handling methods by performing various data processing such as image preprocessing, image generating, and lmdb conversion of various text-in-the-wild image and other documents. In addition, it was a valuable opportunity to improve image and natural language problem solving skills by training with tuning, analyzing and correcting problems in the results. However, due to hardware limitations, all the collected data could not be used, and even training had to be forcibly terminated, resulting in lower accuracy than expected. We are sure that better results would be obtained if all epochs were completely trained using all the data in a better hardware environment, and We are planning to train our trained model in better hardware environment. If Korean OCR using text-in-the-wild develops further, the time and effort to detect and recognize text in images will be greatly reduced, and productivity can be increased by minimizing the process required to convert text images into data and automating the process.

## Reference


## Developer
Hyunji Kim, Yeaji Kim, Changhyeon Lee.
<br />
Hyunji Kim <a href="mailto:hjk021@khu.ac.kr"> <img src ="https://img.shields.io/badge/Gmail-EA4335.svg?&style=flat-squar&logo=Gmail&logoColor=white"/> 
[<img src="https://img.shields.io/badge/Notion-000000?style=flat-square&logo=Notion&logoColor=white"/>](https://read-me.notion.site/Hyunji-Kim-9dbdb62cc84347feb85b3c58225bb63b)
	<a href = "https://github.com/HJK02130"> <img src ="https://img.shields.io/badge/Github-181717.svg?&style=flat-squar&logo=Github&logoColor=white"/> </a>
